
\newpage
\section{Unfitted cut discontinuous Galerkin method for the Poisson problem}%
\label{sec:elliptic}

\subsection{Poisson problem}%
\label{sub:possion_problem}
We will first consider the continuous Poisson problem. Let $f \in H^1( \Omega ) $ and $g \in H^{\frac{1}{2}}( \Gamma ) $ and $\Omega  \in \mathbb{R} ^{d}$ . We then define the strong formulation of the Possion problem to be \[
\begin{split}
    -\Delta u &= f \quad  \text{in }\Omega  \\
     u &= g  \quad \text{on } \Gamma    \\
\end{split} .
\]
Let us define the Hilbert spaces $V=H^{1}( \Omega ) $,   $V_{g} = \left\{ v \in H^{1}( \Omega ): v \mid _{\Gamma } = g \right\} $, the bilinear form $a: V \times V  \to \mathbb{R}  $ and the linear form $l: V'\to \mathbb{R}  $ s.t. \[
a( u,v) = ( \nabla u, \nabla v) _{\Omega }, \quad l( v) = (f,v)_{\Omega }.
\]
We say the weak formulation is to find a $u \in V_{g}$ so this equation holds  \[
a( u,v) = l( v), \quad  \forall v \in V
\]
\subsection{CutFEM }%
\label{sub:cutfem}

One of the key element to unfitted methods is that is relying on a background mesh. Let $\widetilde{\Omega }$ as a background domain with a corresponding shape regular and quasi-uniform background mesh $\widetilde{\mathcal{T}}_{h} $. We will assume we
have a physical domain $\Omega
\subset \widetilde{\Omega }$ with a corresponding $\Gamma \in C^2 $ boundary. For $\mathcal{T}_{h} $ we define a the so-called active mesh
$\mathcal{T} _{h} \subset \widetilde{\mathcal{T} }_{h}$ consisting of those elements that intersect with the interior of $\Omega^{\circ } = \Omega  \setminus \Gamma   $. That is,\[
\mathcal{T }_{h} = \left\{ T \in  \mathcal{T}_{h}  \mid  T \cap \Omega \neq \emptyset   \right\}.
\]
The set of interior facets in $\mathcal{T}_{h} $ is defined as $\mathcal{F}_{h} $.
Let the submesh $\mathcal{T}_{\Gamma } \subset \mathcal{T}_{h}  $ consisting of all cut elements, \[
\mathcal{T} _{\Gamma  } = \left\{ T \in \mathcal{T} _{h}  \mid  T \cap  \Gamma  \neq \emptyset  \right\}.
\]

\todo[inline]{ Wrap the above definitions into definitions blocks.}

We denote the discrete to function space $V_{h}$ to be the broken polynomial space of order $k$, that is, $V_{h} := \mathcal{P}^{k}( \mathcal{T}_{h} )  $.


Let the bilinear form $a_{h}: V_{h} \times V_{h} \to \mathbb{R} $  and the linear form $l_{h}: V_{h} \to \mathbb{R} $. We denote the symmetric interior penalty discontinuous Galerkin Poisson (DG Poisson)  formulation to be

\begin{equation}
\label{eq:poisson_DG}
\begin{split}
    a_{h}( v,w)  = &( \nabla v, \nabla w)_{\mathcal{T} _{h} \cap \Omega } - ( \partial _{n} v,w)_{\Gamma } - ( v, \partial _{n} w)_{\Gamma } + \beta ( h^{-1} v,w)_{\Gamma } \\
    & - ( \mean{ \partial _{n} v }  , \jump{ w }  ) _{\mathcal{F} _{h} \cap \Omega } - ( \jump{ v }  , \mean{ \partial _{n} w }  )_{\mathcal{F} _{h}\cap \Omega } + \beta ( h^{-1} \jump{ v }  , \jump{ w }  )_{\mathcal{F}_{h}\cap \Omega  } \\
    l_{h}( v)  &=  ( f,v) _{\mathcal{T} _{h} \cap \Omega } - ( \partial _{n} v,g) _{\Gamma } + \beta ( h^{-1} g,v)_{\Gamma }
\end{split}
\end{equation}
For more information about the derivation, see \cite[Chapter 4.2]{pietro2012}. Remark that this formulation is defined on unfitted meshed and, thus, imposing Nitsche penalty to fulfill the boundary condition.
\todo[inline]{ Need source of the derivation or maybe need to to it myself }
However, to show well-posedness will we later see that it is necessary to add a stability term . Hence, the symmetric interior penalty cut discontinuous Galerkin Poisson (CutDG Poisson) formulation,
\[
A_{h}( u, v) := a_{h}( u, v) + g_{h}( u_{h}, v) = l_{h}(v)
\]
We denote the stability term $g_{h}: V_{h} \times V_{h} \to \mathbb{R} $ as the so-called ghost penalty term.

\begin{definition}[CutDG Poisson problem]
    \label{def:cutdg_poisson_problem}

We denote the CutDG Poisson problem as follows; to find a unique $u_{h} \in V_{h}$ s.t.  \[
A_{h}(u_{h}, v ) = l_{h}( v)  \quad \forall  v \in V_{h}.
\]
\end{definition}

Our main goal is to determine the necessary stability criteria for the ghost penalty term for the CutDG Poisson problem to be well-posed. We can then apply these criteria to engineer a ghost penalty to handle this problem. For convenience will we make some assumptions on our problem.


\begin{assumption}[G1]
    \label{as:G1}
    The boundary $\Gamma $ is $C^{2}$.
\end{assumption}

\begin{remark}
In most applications is $\Gamma $ represented with a level-set function, that is a sufficiently smooth function $\phi ( x)  = 0$. There exists several methods to discretize this properly s.t. it can we can compute the integral over the cut elements.
    However, to simplify the analysis will we assume the numerical contributions of the cut elements consisting of $\mathcal{T}_{h} \cap \Gamma   $ and $\mathcal{F}_{h} \cap \Omega  $ to be exact.
\end{remark}


\begin{assumption}[G1]
   The active mesh $\mathcal{T}_{h} $ is mesh conform, quasi-uniform and shape regular.
\end{assumption}

\begin{assumption}[G3]
    For $T \in \mathcal{T} _{\Gamma   }$ there is a path $P$ of $diam(P) \lesssim h$ which contains $T$ and an element $T'$ with a "fat" interaction satisfying $\left\lvert T' \cap \Omega  \right\rvert \ge c_{s} \left\lvert T'  \right\rvert _{d}$
    \todo[inline]{ Don't understand this definition}
    \todo[inline]{ TODO: Define patch in mathematical background. }
\end{assumption}


\subsection{Well-posedness}%
\label{sub:well_posedness}

Our goal is to show that the problem \ref{def:cutdg_poisson_problem} is well posed. We can do this by applying Lax-Milgram theorem \ref{def:lax-milgram} by showing that the bilinear form $A_{h}$ is bounded and coercive.

First of all, to be able to prove well-posedness we need proper norms for $A_{h}$ .
That is, let $v \in V_{h}$ we then define the following norms,
\[
\begin{split}
    \| v \|_{ a_{h} }^{ 2 }  & = \| \nabla v \|_{ \mathcal{T} _{h} \cap \Omega  }^{ 2 } + \| h^{-\frac{1}{2}} \jump{ v }   \|_{ \mathcal{F} _{h} \cap \Omega   }^2 + \| h^{-\frac{1}{2} } v \|_{ \Gamma  }^{ 2 }   \\
    \left\lvert v \right\rvert _{g_{h}}^{2} & = g_{h}( v,v)  \\
    \| v \|_{ A_{h} }^{ 2 } &= \| v \|_{ a_{h} }^{ 2 } + \left\lvert v \right\rvert_{g_{h}}^{2}     \\
\end{split}
\]
While for $v \in H^2( \mathcal{T} _{h}) + V_{h}$ we define the following norm,\[
\| v \|_{ a_{h},* }^{ 2 } = \| v \|_{ a_{h} }^{ 2 } +   \| h^{\frac{1}{2}} \jump{ \partial _{n} v }    \|_{  \mathcal{F} _{h} \cap \Omega }^{  2} + \| h^{\frac{1}{2}} \partial _{n} v \|_{ \Gamma  }^{ 2 }.
\]

Before we prove well-posedness will we first establish some important inequalities which will be used in the proof.
Recall the much known local inverse estimate in \ref{lemma:local_inverse_estimates}. In the case of unfitted methods the corresponding inverse estimates are,
\begin{equation}
\label{eq:unfitted_localinv}
\begin{split}
    \| \partial _{n} v \|_{ F\cap \Omega  }^{  } & \le \| \partial _{n}  v \|_{F  }^{  } \le  C_{I} h_{T} ^{-\frac{1}{2}} \| \nabla v \|_{ T }^{  } \\
    \| \partial _{n} v \|_{ T\cap \Gamma   }^{  } &  \le  C h_{T} ^{-\frac{1}{2}} \| \nabla v \|_{ T }^{  } \\
\end{split}
\end{equation}


Again, the first inequality is a quite trivial extension of the original estimate. However, the second inequality may be cumbersome to prove. For a proof, see \cite{hansbo2003finite}.
An useful extension is to generalize this equality for the full mesh $\mathcal{T} _{h}$.
Remark that the $C_{\Gamma } $ is dependent on the local curvature of $\Gamma $. \todo[inline]{ TODO: Look into\cite{hansbo2003finite} and actually find the proof.}
We will now present a slightly generalized version.
\begin{corollary}[Unfitted inverse estimates]
    \label{cor:unfitted_inverse_estimates}
    Assume that $v \in V_{h} $ and let $\mathcal{T}_{h} $ be a active mesh. Then the following estimates holds,
    \[
\begin{split}
    \| \mean{ \partial _{n} v }   \|_{ \mathcal{F}_h \cap \Omega  }^{  } & \le   C_{I} h ^{-\frac{1}{2}} \| \nabla v \|_{ \mathcal{T}_{h}  }^{  } \\
    \| \partial _{n} v \|_{ \mathcal{T}_{h} \cap \Gamma   }^{  } &  \le  C_{\Gamma } h ^{-\frac{1}{2}} \| \nabla v \|_{ \mathcal{T} _{h} }^{  } \\
\end{split}
    \]
    Or equivalently,
    \[
\begin{split}
    h^{\frac{1}{2}}\| \mean{ \partial _{n} v } \|_{ \mathcal{F}_h \cap \Omega  }^{  } & \le  C_{I}  \| \nabla v \|_{ \mathcal{T}_{h}  }^{  } \\
    h^{\frac{1}{2}}\| \partial _{n} v \|_{ \mathcal{T}_{h} \cap \Gamma   }^{  } &  \le  C_{\Gamma }  \| \nabla v \|_{ \mathcal{T} _{h} }^{  } \\
\end{split}
    \]
Where $C_{I}$ depends on the dimension $d$, the degree $k$ and shape regularity of $T$. Compared to $C_{I}$ the constant $C_{\Gamma }$ also depends on the curvature of $\Gamma $.
\end{corollary}


To be able to apply the \eqref{eq:unfitted_localinv} globally can we make the following assumption for the ghost penalty term.

\begin{assumption}[EP1]
    \label{as:est_EP1}
    Let $\Omega $ be the physical domain to the active mesh $\mathcal{T} _{h}$. The ghost penalty is a symmetric, positive semi-definite bilinear form, and satisfies the following inequality  \[
    \| \nabla v \|_{ \mathcal{T}_{h}  }^{ 2 }  \le C_{g} ( \| \nabla v \|_{ \Omega  }^{ 2 }  + \left\lvert v  \right\rvert _{ g_{h} }^{ 2 } ) \forall v \in  V_{h}
    \]
    Here the constant $C_{g}$ depends on the diameter $h$, the order $k $ and the dimension $d$.
\end{assumption}
\begin{remark}
Note that since we assume the ghost penalty is to be a symmetric, positive semi-definite bilinear term, the Cauchy-Schwarz inequality holds, that is,\[
g_{h}( v,u)  \le \abs{ v } _{g_{h}} \abs{ u }_{g_{h}} \forall v,u \in V_{h}.
\]
\end{remark}


\begin{corollary}
    \label{cor:g_h_inverste_results}
    Let $g_{h}$ satisfy the assumption \ref{as:est_EP1} and applying the ideas in \eqref{eq:unfitted_localinv}, then the following estimate holds true.
    \[
    \| h^{\frac{1}{2}} \partial _{n} v \|_{ \Gamma   }^{ 2 } + \| h^{\frac{1}{2}} \partial _{n} v \|_{ \mathcal{F} _{h} \cap \Omega  }^{ 2 }  \le  C_{g}( C_{\Gamma } + C_{I}) ( \| \nabla v  \|_{ \Omega   }^{2  }
     + \left\lvert v \right\rvert _{g_{h}}^{2}) \le C \| v \|_{ A_{h} }^{ 2 }
    \]
    We can observe that $C$ depends on the diameter $h$, the order $k $ and the dimension $d$.
\end{corollary}


Finally being able to control the estimates on cut elements can we start showing well-posedness for the formulation.

\begin{lemma}
    $A_{h}$ is coercive, i.e., satisfying the general definition \ref{def:coercivity}.
\end{lemma}
\begin{proof}
We want to show that $  \| v \|_{A_{h}}^{2}   \lesssim A_{h}( v,v) \forall v \in V_{h} $.
    Assume that $v \in V_{h} $. Then does in short this inequality hold,
\[
    \begin{split}
A_{h}( v, v)   =& a_{h}( v, v) + \abs{ v }  _{g_{h}}^{2} \\
=&  \| \nabla v \|_{  \mathcal{T}_{h} \cap \Omega }^{ 2 } + \abs{v  }  _{ g_{h} }^{ 2 } + \beta \| h^{-\frac{1}{2}} \jump{ v }   \|_{\mathcal{F} _{h} \cap \Omega   }^{  2}   + \beta \| h^{-\frac{1}{2}}  v    \|_{ \Gamma}^{  2}
\\ & - 2( \mean{ \partial _{n} v  },  \jump{ v })_{\mathcal{F} _{h} \cap \Omega }
\\ & - 2 ( \partial _{n} v,v)_{\Gamma \cap \Omega }     \\
\ge  &  \| \nabla v \|_{  \mathcal{T}_{h} \cap \Omega }^{ 2 } + \left\lvert v \right\rvert_{ g_{h} }^{ 2 } + \beta \| h^{-\frac{1}{2}} \jump{ v }   \|_{\mathcal{F} _{h} \cap \Omega   }^{2  }   + \beta \| h^{-\frac{1}{2}}  v    \|_{ \Gamma}^{  2}
\\ & - \varepsilon \| h^{\frac{1}{2}}  \mean{  \partial _{n} v}      \|_{ \mathcal{F} _{h} \cap \Omega  }^{2  } - \varepsilon ^{-1} \| h^{-\frac{1}{2}} \jump{ v }   \|_{\mathcal{F} _{h} \cap \Omega   }^{ 2 }
\\ & - \varepsilon \| h^{\frac{1}{2}} \partial _{n} v \|_{ \Gamma \cap \Omega  }^{2  } -  \varepsilon ^{-1} \| h^{-\frac{1}{2}} v \|_{  \Gamma \cap  \Omega }^{ 2 } \\
\ge & ( 1-\varepsilon C)  ( \| \nabla v \|_{\Omega   }^{2  } + \left\lvert v \right\rvert _{g_{h}}   ) + ( \beta - \varepsilon ^{-1} ) ( \| h^{-\frac{1}{2}} \jump{ v }   \|_{ \mathcal{F} _{h}\cap  \Omega  }^{ 2 } + \| h^{\frac{1}{2}} v \|_{\Gamma
}^{  2}  )    \\
\ge & \frac{1}{2} \| v \|_{ A_{h} }^{ 2 }
    \end{split}
\]
where we used $C= C_{g} ( C_{\Gamma } + C_{I}) $ and chose $\varepsilon = \frac{1}{2C}$ and $\beta  = 4C$.
\todo[inline]{ Why use the notation $\Gamma \cap \Omega $ in the paper? It is equivalent with $\Gamma $.   }

Remark that we in the second step we expanded $ 1 =  h^{\frac{1}{2}} h^{-\frac{1}{2}}$ and used the Youngs $\varepsilon $-inequality \ref{lemma:youngs_epsilon} s.t.,
 \[
     \begin{split}
         - 2( \mean{ \partial _{n} v  },  \jump{ v })_{\mathcal{F} _{h} \cap \Omega } &\ge - \varepsilon \| h^{\frac{1}{2}}  \mean{  \partial _{n} v}      \|_{ \mathcal{F} _{h} \cap \Omega  }^{2  }
  - \varepsilon ^{-1} \| h^{-\frac{1}{2}} \jump{ v }   \|_{\mathcal{F} _{h} \cap \Omega   }^{ 2 } \\
- 2 ( \partial _{n} v,v)_{\Gamma \cap \Omega }   &\ge- \varepsilon \| h^{\frac{1}{2}} \partial _{n} v \|_{ \Gamma \cap \Omega  }^{2  }
-  \varepsilon ^{-1} \| h^{-\frac{1}{2}} v \|_{  \Gamma \cap  \Omega }^{ 2 }
     \end{split}
 \]

 On the third step we first rearranged the terms, that is,
 \[
     \begin{split}
   \| \nabla v \|_{ \mathcal{T}_{h} \cap  \Omega }^{ 2 } & + \left\lvert v \right\rvert_{ g_{h} }^{ 2 } + \beta \| h^{-\frac{1}{2}} \jump{ v }   \|_{\mathcal{F} _{h} \cap \Omega   }^{  }   + \beta \| h^{-\frac{1}{2}}  v    \|_{ \Gamma}^{  }
\\  &- \varepsilon \| h^{\frac{1}{2}}  \mean{  \partial _{n} v}      \|_{ \mathcal{F} _{h} \cap \Omega  }^{2  } - \varepsilon ^{-1} \| h^{-\frac{1}{2}} \jump{ v }   \|_{\mathcal{F} _{h} \cap \Omega   }^{ 2 }
\\ &-  \varepsilon \| h^{\frac{1}{2}} \partial _{n} v \|_{ \Gamma \cap \Omega  }^{2  } -  \varepsilon ^{-1} \| h^{-\frac{1}{2}} v \|_{  \Gamma \cap  \Omega }^{ 2 } \\
&=  \| \nabla v \|_{ \mathcal{T}_{h} \cap  \Omega }^{ 2 }  + \left\lvert v \right\rvert_{ g_{h} }^{ 2 } \\
&\quad  + ( \beta \| h^{-\frac{1}{2}} v \|_{\Gamma   }^{  } - \varepsilon ^{ -1}\| h^{-\frac{1}{2}} v \|_{\Gamma \cap \Omega    }^{  } )  \\
&\quad  -  \varepsilon ( \| h^{\frac{1}{2}} \mean{ \partial _{n} v }    \|_{  \mathcal{F}_{h} \cap \Omega  }^{  } + \| h^{\frac{1}{2}} \partial _{n} v \|_{ \Gamma \cap \Omega  }^{  }  ) \\
&\quad  - \varepsilon ^{-1} ( \| h^{-\frac{1}{2}} \jump{ v }   \|_{ \mathcal{F}_{h} \cap \Omega   }^{2  } + \| h^{-\frac{1}{2}} v \|_{ \Gamma \cap \Omega   }^{  }  )
     \end{split}
 \]
 And then we $\ldots$.
 \\
 \todo[inline]{ Need to finish this proof. }

\end{proof}

Another important recipe before we can use Lax-Milgram is that the we need to show that $A_{h} $ is a bilinear bounded operator.

\begin{lemma}
    $A_{h}$ is bounded, i.e.,
    $$ \left\lvert A_{h}( v,u) \right\rvert \lesssim \| v \|_{A_{h}  }^{  }     \| u \|_{A_{h}  }^{  } \quad  \forall v,u \in V _{h}, $$
    whenever $\beta $ is large enough. Moreover, for $v \in  H^{2}( \mathcal{T}_{h} ) + V_{h} $
    and $w \in V_{h}$ satisfies \[
    \left\lvert a_{h}( v,w) \right\rvert  \lesssim \| v \|_{ a_{h},* }^{  }  \| w \|_{ a_{h},* }^{  }
    \]
\end{lemma}

\begin{proof}
 We will then show that
$a_{h}( v,u) \lesssim \| v \|_{ a_{h},* }^{  }  \| u \|_{ a_{h},* }^{  }$
for $ \forall v \in  H^{2}( \mathcal{T}_{h} ) + V_{h} $ and $ \forall u \in V_{h}$.

    \begin{enumerate}[label=\arabic*)]
        \item
The goal is to show that $\left\lvert A_{h}( v,u) \right\rvert \lesssim \| v \|_{A_{h}  }^{  }     \| u \|_{A_{h}  }^{  }  \forall v,u \in V_{h} $.
            By definition we know that \[
    \left\lvert A_{h}( v,u) \right\rvert \le  \left\lvert a_{h}( v,u)  \right\rvert  + \left\lvert g_{h}( v,u)  \right\rvert,
    \]
    thus, it is only necessary to show that each of those terms are bounded.

         First of all, by assumption does the ghost penalty satisfy the Cauchy-Schwartz inequality, that is,  \[
\left\lvert g_{h}( v,u)  \right\rvert \le \left\lvert v \right\rvert _{g_{h}} \left\lvert u \right\rvert _{g_{h}} \le \| v \|_{ A_{h} }^{  } \| u \|_{ A_{h} }^{  }
    \]

    Similarly, we can see that,      \[
        \begin{split}
    \left\lvert a_{h}( v,u)  \right\rvert & \le  \left\lvert( \nabla v, \nabla u)_{\mathcal{T} _{h} \cap \Omega }  \right\rvert +  \left\lvert  ( \partial _{n} v,u)_{\Gamma } \right\rvert   + \left\lvert ( v, \partial _{n} u)_{\Gamma } \right\rvert  +
    \left\lvert\beta ( h^{-1} v,u)_{\Gamma }  \right\rvert  \\
    & \quad  + \left\lvert( \mean{ \partial _{n} v }  , \jump{ u }  ) _{\mathcal{F} _{h} \cap \Omega }  \right\rvert  + \left\lvert ( \jump{ v }  , \mean{ \partial _{n} u }  )_{\mathcal{F} _{h}\cap \Omega } \right\rvert  + \left\lvert \beta ( h^{-1} \jump{ v }  ,
    \jump{ u }  )_{\mathcal{F}_{h}\cap \Omega  } \right\rvert \\
     & \lesssim  \| u \|_{ A_{h} }^{  } \| v \|_{ A_{h} }^{  }
        \end{split}
    \]

This is proved by showing that each term is bounded by applying the Cauchy-Schwartz inequality, Assumption \ref{as:est_EP1} and Corollary \ref{cor:g_h_inverste_results} and \ref{cor:unfitted_inverse_estimates}. That is, here is some
    detailed calculations,
    \begin{itemize}
        \item For the first terms we can see that \[
\begin{split}
\left\lvert( \nabla v, \nabla u)_{\mathcal{T} _{h} \cap \Omega }  \right\rvert & \le \| \nabla v \|_{ \mathcal{T} _{h}\cap \Omega  }^{  }\| \nabla u \|_{ \mathcal{T} _{h} \cap \Omega  }^{  } \\
& \le \| \nabla v \|_{ \mathcal{T} _{h}  }^{  }\| \nabla u \|_{ \mathcal{T} _{h}   }^{  } \\
        & \lesssim   ( \| \nabla v \|_{ \Omega  }^{  } + \abs{ v } _{g_{h}} )( \| \nabla u \|_{ \Omega  }^{  } + \abs{ u } _{g_{h}} )\\
        &  \lesssim  \| v \|_{ A_{h} }^{  } \|u  \|_{ A_{h} }^{  }
\end{split}
            \]

        \item For the second and third terms along the boundary we get,
            \[
                \begin{split}
                 \left\lvert ( \partial _{n} v,  u)_{\Gamma } \right\rvert & \le \|h^{\frac{1}{2}} \partial _{n}v  \|_{ \Gamma   }^{  } \| h^{-\frac{1}{2}} u  \|_{ \Gamma  }^{  }  \\
                 & \le \| \nabla v \|_{\mathcal{T}_{h}   }^{  } \|h^{-\frac{1}{2}} u \|_{ \Gamma    }^{  } \\
 &  \le ( \| \nabla v \|_{ \Omega  }^{  } + \abs{ v } _{g_{h}} ) \|h^{-\frac{1}{2}} u \|_{ \Gamma    }^{  } \\
                 & \lesssim   \| v \|_{A_{h}   }^{  } \| u \|_{ A_{h}    }^{  }
                \end{split}
            \]

        \item The boundary penalty term
            \[
                \begin{split}
                    \left\lvert\beta ( h^{-1} v,u)_{\Gamma }  \right\rvert  & \le \beta \| h^{-\frac{1}{2}} v \|_{\Gamma   }^{  } \| h^{-\frac{1}{2}} u \|_{\Gamma   }^{  } \\
                                                                            &\lesssim \| u \|_{\mathcal{A}_{h}   }^{  }\| v \|_{\mathcal{A}_{h}   }^{  }
                \end{split}
            \]

        \item For the jump terms \[
                \begin{split}
                    \left\lvert( \mean{ \partial _{n} v }  , \jump{ u }  ) _{\mathcal{F} _{h} \cap \Omega }  \right\rvert &\lesssim  \| h^{\frac{1}{2}} \mean{ \partial _{n} v }   \|_{ \mathcal{F}_{h}\cap \Omega   }^{  } \|h^{-\frac{1}{2}} \jump{ u }   \|_{ \mathcal{F}_{h}\cap
                    \Omega   }^{  } \\
&\lesssim  \| u   \|_{ A_{h}    }^{  } \|  u    \|_{ A_{h}   }^{  }
                \end{split}
        \]
        \todo[inline]{ Find a way to argue  $\| \mean{ \partial _{n} v }  \|_{\mathcal{F} _{h} \cap \Omega   }^{  } $}

    \item And for the DG penalty term
        \[
            \begin{split}
         \left\lvert \beta ( h^{-1} \jump{ v }  ,
         \jump{ u }  )_{\mathcal{F}_{h}\cap \Omega  } \right\rvert &\le  \beta \| h^{-\frac{1}{2}    }\jump{ v } \|_{ \mathcal{F}_{h} \cap \Omega  }^{  }\| h^{-\frac{1}{2}    }\jump{ u } \|_{\mathcal{F}_{h} \cap \Omega   }^{  } \\
    &\lesssim \| v \|_{A_{h}  }^{  } \| u \|_{A_{h}  }^{  }
            \end{split}
        \]
        Hence, the statement $\left\lvert A_{h}( v,u) \right\rvert \lesssim \| v \|_{A_{h}  }^{  }     \| u \|_{A_{h}  }^{  }  \forall v,u \in V_{h} $ holds.
    \end{itemize}

\item We want to show $a_{h}( v,u) \lesssim \| v \|_{ a_{h},* }^{  }  \| u \|_{ a_{h},* }^{  }$
for $ \forall v \in  H^{2}( \mathcal{T}_{h} ) + V_{h} $ and $ \forall u \in V_{h}$.
    \end{enumerate}


\end{proof}


\subsection{Constructing ghost penalties}%
\label{sub:constructing_ghost_penalties}



To show well-posedness have we used the following assumptions of $g_{h}$

\begin{assumption}[EP1]
    \label{as:EP1_2}
    $H^{1}$ semi-norm extension property for $v \in  V_{h}$,  \[
    \| \nabla v \|_{ \mathcal{T} _{h} }^{  }  \lesssim \| \nabla v \|_{ \Omega  }^{  }  + \abs{ v } _{g_{h}}
    \]
\end{assumption}

\begin{assumption}[EP2]
    \label{as:EP2}
    Weak consistency for $v \in H^{s}( \Omega ) $ and $r = \min\{s, k+1\} $, \[
    \abs{ \pi ^{e}_{h} v }_{g_{h}} \lesssim  h^{r-1} \| v \|_{ r,\Omega  }^{  }
    \]
\end{assumption}

\begin{assumption}[EP3]
    \label{as:EP3}
    $L^{2}$ norm extension property for $v \in V_{h}$,\[
    \| v \|_{ \mathcal{T} _{h} }^{  } \lesssim \| v  \|_{H_{h}  }^{  }   + \abs{ v } _{g_{h}}
    \]
\end{assumption}

\begin{assumption}[EP4]
    \label{as:EP4}
    Inverse inequality for $v \in  V_{h}$,
    \[
    \abs{ v } _{g_{h}} \lesssim h^{-1} \| v \|_{ \mathcal{T} _{h} }^{  } .
    \]
\end{assumption}


\todo[inline]{ I am not proving condition numbers in this report. Should I still use all the assumptions and refer to the paper? }
\todo[inline]{TODO: Define norm $\| \cdot  \|_{ r,\Omega  }^{  } $ in mathematical background.  }

\subsubsection{Face-based ghost penalties}%
\label{ssub:face_based_ghost_penalties}

\subsubsection{Projection-based ghost penalties}%
\label{ssub:projection_based_ghost_penalties}



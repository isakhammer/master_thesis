
\newpage
\section{Unfitted cut discontinuous Galerkin method for the Poisson problem}%
\label{sec:elliptic}

\subsection{Poisson problem}%
\label{sub:possion_problem}
We will first consider the continuous Poisson problem. Let $f \in H^1( \Omega ) $ and $g \in H^{\frac{1}{2}}( \Gamma ) $ and $\Omega  \in \mathbb{R} ^{d}$ . We then define the strong formulation of the Possion problem to be \[
\begin{split}
    -\Delta u &= f \quad  \text{in }\Omega  \\
     u &= g  \quad \text{on } \Gamma    \\
\end{split} .
\]
Let us define the Hilbert spaces $V=H^{1}( \Omega ) $,   $V_{g} = \left\{ v \in H^{1}( \Omega ): v \mid _{\Gamma } = g \right\} $, the bilinear form $a: V \times V  \to \mathbb{R}  $ and the linear form $l: V'\to \mathbb{R}  $ s.t. \[
a( u,v) = ( \nabla u, \nabla v) _{\Omega }, \quad l( v) = (f,v)_{\Omega }.
\]
We say the weak formulation is to find a $u \in V_{g}$ so this equation holds
\begin{equation}
    \label{eq:possion_weak}
a( u,v) = l( v)  , \quad  \forall v \in V
\end{equation}
\subsection{CutFEM }%
\label{sub:cutfem}

One of the key element to unfitted methods is that is relying on a background mesh. Let $\widetilde{\Omega }$ as a background domain with a corresponding shape regular and quasi-uniform background mesh $\widetilde{\mathcal{T}}_{h} $. We will assume we
have a physical domain $\Omega
\subset \widetilde{\Omega }$ with a corresponding $\Gamma \in C^2 $ boundary. For $\mathcal{T}_{h} $ we define a the so-called active mesh
$\mathcal{T} _{h} \subset \widetilde{\mathcal{T} }_{h}$ consisting of those elements that intersect with the interior of $\Omega^{\circ } = \Omega  \setminus \Gamma   $. That is,\[
\mathcal{T }_{h} = \left\{ T \in  \mathcal{T}_{h}  \mid  T \cap \Omega \neq \emptyset   \right\}.
\]
The set of interior facets in $\mathcal{T}_{h} $ is defined as $\mathcal{F}_{h} $.
Let the submesh $\mathcal{T}_{\Gamma } \subset \mathcal{T}_{h}  $ consisting of all cut elements, \[
\mathcal{T} _{\Gamma  } = \left\{ T \in \mathcal{T} _{h}  \mid  T \cap  \Gamma  \neq \emptyset  \right\}.
\]


We denote the discrete to function space $V_{h}$ to be the broken polynomial space of order $k$, that is, $V_{h} := \mathcal{P}^{k}( \mathcal{T}_{h} )  $.


Let the bilinear form $a_{h}: V_{h} \times V_{h} \to \mathbb{R} $  and the linear form $l_{h}: V_{h} \to \mathbb{R} $. We denote the symmetric interior penalty discontinuous Galerkin Poisson (DG Poisson)  formulation to be

\begin{equation}
\label{eq:poisson_DG}
\begin{split}
    a_{h}( v,w)  = &( \nabla v, \nabla w)_{\mathcal{T} _{h} \cap \Omega } - ( \partial _{n} v,w)_{\Gamma } - ( v, \partial _{n} w)_{\Gamma } + \beta ( h^{-1} v,w)_{\Gamma } \\
    & - ( \mean{ \partial _{n} v }  , \jump{ w }  ) _{\mathcal{F} _{h} \cap \Omega } - ( \jump{ v }  , \mean{ \partial _{n} w }  )_{\mathcal{F} _{h}\cap \Omega } + \beta ( h^{-1} \jump{ v }  , \jump{ w }  )_{\mathcal{F}_{h}\cap \Omega  } \\
    l_{h}( v)  &=  ( f,v) _{\mathcal{T} _{h} \cap \Omega } - ( \partial _{n} v,g) _{\Gamma } + \beta ( h^{-1} g,v)_{\Gamma }
\end{split}
\end{equation}
For more information about the derivation, see \cite[Chapter 4.2]{pietro2012}. Remark that this formulation is defined on unfitted meshed and, thus, imposing Nitsche penalty to fulfill the boundary condition.
\todo[inline]{ Need source of the derivation or maybe need to to it myself }
However, to show well-posedness will we later see that it is necessary to add a stability term . Hence, the symmetric interior penalty cut discontinuous Galerkin Poisson (CutDG Poisson) formulation,
\[
A_{h}( u, v) := a_{h}( u, v) + g_{h}( u_{h}, v) = l_{h}(v)
\]
We denote the stability term $g_{h}: V_{h} \times V_{h} \to \mathbb{R} $ as the so-called ghost penalty term.

\begin{definition}[CutDG Poisson problem]
    \label{def:cutdg_poisson_problem}

We denote the CutDG Poisson problem as follows; to find a unique $u_{h} \in V_{h}$ s.t.  \[
A_{h}(u_{h}, v ) = l_{h}( v)  \quad \forall  v \in V_{h}.
\]
\end{definition}

Our main goal is to determine the necessary stability criteria for the ghost penalty term for the CutDG Poisson problem to be well-posed. We can then apply these criteria to engineer a ghost penalty to handle this problem. For convenience will we make some assumptions on our problem.



\begin{assumption}[G1]
    \label{as:G1}
    The boundary $\Gamma $ is $C^{2}$.
\end{assumption}

\begin{remark}
In most applications is $\Gamma $ represented with a level-set function, that is a sufficiently smooth function $\phi ( x)  = 0$. There exists several methods to discretize this properly s.t. it can we can compute the integral over the cut elements.
    However, to simplify the analysis will we assume the numerical contributions of the cut elements consisting of $\mathcal{T}_{h} \cap \Gamma   $ and $\mathcal{F}_{h} \cap \Omega  $ to be exact.
\end{remark}


\begin{assumption}[G2]
    \label{as:G2}
   The active mesh $\mathcal{T}_{h} $ is mesh conform, quasi-uniform and shape regular.
\end{assumption}

\begin{assumption}[G3]
    \label{as:G3}
    For $T \in \mathcal{T} _{\Gamma   }$ there is a path $P$ of $diam(P) \lesssim h$ which contains $T$ and an element $T'$ with a "fat" interaction satisfying $\left\lvert T' \cap \Omega  \right\rvert \ge c_{s} \left\lvert T'  \right\rvert _{d}$
    \todo[inline]{ Don't understand this definition}
    \todo[inline]{ TODO: Define patch in mathematical background. }
\end{assumption}


\subsection{Well-posedness}%
\label{sub:well_posedness}

Our goal is to show that the problem \ref{def:cutdg_poisson_problem} is well posed. We can do this by applying Lax-Milgram theorem \ref{def:lax-milgram} by showing that the bilinear form $A_{h}$ is bounded and coercive.

First of all, to be able to prove well-posedness we need proper norms for $A_{h}$ .
That is, let $v \in V_{h}$ we then define the following norms,
\[
\begin{split}
    \| v \|_{ a_{h} }^{ 2 }  & = \| \nabla v \|_{ \mathcal{T} _{h} \cap \Omega  }^{ 2 } + \| h^{-\frac{1}{2}} \jump{ v }   \|_{ \mathcal{F} _{h} \cap \Omega   }^2 + \| h^{-\frac{1}{2} } v \|_{ \Gamma  }^{ 2 }   \\
    \left\lvert v \right\rvert _{g_{h}}^{2} & = g_{h}( v,v)  \\
    \| v \|_{ A_{h} }^{ 2 } &= \| v \|_{ a_{h} }^{ 2 } + \left\lvert v \right\rvert_{g_{h}}^{2}     \\
\end{split}
\]
While for $v \in H^2( \mathcal{T} _{h}) + V_{h}$ we define the following norm,\[
\| v \|_{ a_{h},* }^{ 2 } = \| v \|_{ a_{h} }^{ 2 } +   \| h^{\frac{1}{2}} \jump{ \partial _{n} v }    \|_{  \mathcal{F} _{h} \cap \Omega }^{  2} + \| h^{\frac{1}{2}} \partial _{n} v \|_{ \Gamma  }^{ 2 }.
\]

Before we prove well-posedness will we first establish some important inequalities which will be used in the proof.
Recall the much known local inverse estimate in \ref{lemma:local_inverse_estimates}. In the case of unfitted methods the corresponding inverse estimates are,
\begin{equation}
\label{eq:unfitted_localinv}
\begin{split}
    \| \partial _{n} v \|_{ F\cap \Omega  }^{  } & \le \| \partial _{n}  v \|_{F  }^{  } \le  C_{I} h_{T} ^{-\frac{1}{2}} \| \nabla v \|_{ T }^{  } \\
    \| \partial _{n} v \|_{ T\cap \Gamma   }^{  } &  \le  C h_{T} ^{-\frac{1}{2}} \| \nabla v \|_{ T }^{  } \\
\end{split}
\end{equation}


Again, the first inequality is a quite trivial extension of the original estimate. However, the second inequality may be cumbersome to prove. For a proof, see \cite{hansbo2003finite}.
An useful extension is to generalize this equality for the full mesh $\mathcal{T} _{h}$.
Remark that the $C_{\Gamma } $ is dependent on the local curvature of $\Gamma $. \todo[inline]{ TODO: Look into\cite{hansbo2003finite} and actually find the proof.}
We will now present a slightly generalized version.
\begin{corollary}[Unfitted inverse estimates]
    \label{cor:unfitted_inverse_estimates}
    Assume that $v \in V_{h} $ and let $\mathcal{T}_{h} $ be a active mesh. Then the following estimates holds,
    \[
\begin{split}
    \| \mean{ \partial _{n} v }   \|_{ \mathcal{F}_h \cap \Omega  }^{  } & \le   C_{I} h ^{-\frac{1}{2}} \| \nabla v \|_{ \mathcal{T}_{h}  }^{  } \\
    \| \partial _{n} v \|_{ \mathcal{T}_{h} \cap \Gamma   }^{  } &  \le  C_{\Gamma } h ^{-\frac{1}{2}} \| \nabla v \|_{ \mathcal{T} _{h} }^{  } \\
\end{split}
    \]
    Or equivalently,
    \[
\begin{split}
    h^{\frac{1}{2}}\| \mean{ \partial _{n} v } \|_{ \mathcal{F}_h \cap \Omega  }^{  } & \le  C_{I}  \| \nabla v \|_{ \mathcal{T}_{h}  }^{  } \\
    h^{\frac{1}{2}}\| \partial _{n} v \|_{ \mathcal{T}_{h} \cap \Gamma   }^{  } &  \le  C_{\Gamma }  \| \nabla v \|_{ \mathcal{T} _{h} }^{  } \\
\end{split}
    \]
Where $C_{I}$ depends on the dimension $d$, the degree $k$ and shape regularity of $T$. Compared to $C_{I}$ the constant $C_{\Gamma }$ also depends on the curvature of $\Gamma $.
        \todo[inline]{ Find a way to argue the $\| \mean{ \partial _{n} v }  \|_{\mathcal{F} _{h} \cap \Omega   }^{  } $ inequality.}
\end{corollary}


To be able to apply the \eqref{eq:unfitted_localinv} globally can we make the following assumption for the ghost penalty term.

\begin{assumption}[EP1]
    \label{as:est_EP1}
    Let $\Omega $ be the physical domain to the active mesh $\mathcal{T} _{h}$. The ghost penalty is a symmetric, positive semi-definite bilinear form, and satisfies the following inequality  \[
    \| \nabla v \|_{ \mathcal{T}_{h}  }^{ 2 }  \le C_{g} ( \| \nabla v \|_{ \Omega  }^{ 2 }  + \left\lvert v  \right\rvert _{ g_{h} }^{ 2 } ) \forall v \in  V_{h}
    \]
    Here the constant $C_{g}$ depends on the diameter $h$, the order $k $ and the dimension $d$.
\end{assumption}
\begin{remark}
Note that since we assume the ghost penalty is to be a symmetric, positive semi-definite bilinear term, the Cauchy-Schwarz inequality holds, that is,\[
g_{h}( v,u)  \le \abs{ v } _{g_{h}} \abs{ u }_{g_{h}} \forall v,u \in V_{h}.
\]
\end{remark}


\begin{corollary}
    \label{cor:g_h_inverste_results}
    Let $g_{h}$ satisfy the assumption \ref{as:est_EP1} and applying the ideas in \eqref{eq:unfitted_localinv}, then the following estimate holds true.
    \[
    \| h^{\frac{1}{2}} \partial _{n} v \|_{ \Gamma   }^{ 2 } + \| h^{\frac{1}{2}} \partial _{n} v \|_{ \mathcal{F} _{h} \cap \Omega  }^{ 2 }  \le  C_{g}( C_{\Gamma } + C_{I}) ( \| \nabla v  \|_{ \Omega   }^{2  }
     + \left\lvert v \right\rvert _{g_{h}}^{2}) \le C \| v \|_{ A_{h} }^{ 2 }
    \]
    We can observe that $C$ depends on the diameter $h$, the order $k $ and the dimension $d$.
\end{corollary}


Finally being able to control the estimates on cut elements can we start showing well-posedness for the formulation.

\begin{lemma}
    $A_{h}$ is coercive, i.e., satisfying the general definition \ref{def:coercivity}.
\end{lemma}
    Here we include a detailed proof based the ideas presented in \cite{gurkan2019stabilized}.
\begin{proof}
We want to show that $  \| v \|_{A_{h}}^{2}   \lesssim A_{h}( v,v) \forall v \in V_{h} $.
    Assume that $v \in V_{h} $. Then does in short this inequality hold,
\[
    \begin{split}
A_{h}( v, v)   =& a_{h}( v, v) + \abs{ v }  _{g_{h}}^{2} \\
=&  \| \nabla v \|_{  \mathcal{T}_{h} \cap \Omega }^{ 2 } + \abs{v  }  _{ g_{h} }^{ 2 } + \beta \| h^{-\frac{1}{2}} \jump{ v }   \|_{\mathcal{F} _{h} \cap \Omega   }^{  2}   + \beta \| h^{-\frac{1}{2}}  v    \|_{ \Gamma}^{  2}
\\ & - 2( \mean{ \partial _{n} v  },  \jump{ v })_{\mathcal{F} _{h} \cap \Omega }
\\ & - 2 ( \partial _{n} v,v)_{\Gamma \cap \Omega }     \\
\ge  &  \| \nabla v \|_{  \mathcal{T}_{h} \cap \Omega }^{ 2 } + \left\lvert v \right\rvert_{ g_{h} }^{ 2 } + \beta \| h^{-\frac{1}{2}} \jump{ v }   \|_{\mathcal{F} _{h} \cap \Omega   }^{2  }   + \beta \| h^{-\frac{1}{2}}  v    \|_{ \Gamma}^{  2}
\\ & - \varepsilon \| h^{\frac{1}{2}}  \mean{  \partial _{n} v}      \|_{ \mathcal{F} _{h} \cap \Omega  }^{2  } - \varepsilon ^{-1} \| h^{-\frac{1}{2}} \jump{ v }   \|_{\mathcal{F} _{h} \cap \Omega   }^{ 2 }
\\ & - \varepsilon \| h^{\frac{1}{2}} \partial _{n} v \|_{ \Gamma \cap \Omega  }^{2  } -  \varepsilon ^{-1} \| h^{-\frac{1}{2}} v \|_{  \Gamma \cap  \Omega }^{ 2 } \\
\ge & ( 1-\varepsilon C)  ( \| \nabla v \|_{\Omega   }^{2  } + \left\lvert v \right\rvert _{g_{h}}   ) + ( \beta - \varepsilon ^{-1} ) ( \| h^{-\frac{1}{2}} \jump{ v }   \|_{ \mathcal{F} _{h}\cap  \Omega  }^{ 2 } + \| h^{\frac{1}{2}} v \|_{\Gamma
}^{  2}  )    \\
\ge & \frac{1}{2} \| v \|_{ A_{h} }^{ 2 }
    \end{split}
\]
where we used $C= C_{g} ( C_{\Gamma } + C_{I}) $ and chose $\varepsilon = \frac{1}{2C}$ and $\beta  = 4C$.
Remark that we in the second step we applied the Youngs $\varepsilon $-inequality \ref{lemma:youngs_epsilon} s.t.,
 \[
     \begin{split}
         - 2( \mean{ \partial _{n} v  },  \jump{ v })_{\mathcal{F} _{h} \cap \Omega } &\ge - \varepsilon \| h^{\frac{1}{2}}  \mean{  \partial _{n} v}      \|_{ \mathcal{F} _{h} \cap \Omega  }^{2  }
  - \varepsilon ^{-1} \| h^{-\frac{1}{2}} \jump{ v }   \|_{\mathcal{F} _{h} \cap \Omega   }^{ 2 } \\
- 2 ( \partial _{n} v,v)_{\Gamma \cap \Omega }   &\ge- \varepsilon \| h^{\frac{1}{2}} \partial _{n} v \|_{ \Gamma \cap \Omega  }^{2  }
-  \varepsilon ^{-1} \| h^{-\frac{1}{2}} v \|_{  \Gamma \cap  \Omega }^{ 2 }
     \end{split}
 \]
 And then applied the Corollary \ref{cor:unfitted_inverse_estimates} and Assumption \ref{as:est_EP1} \[
     \begin{split}
         - \varepsilon \| h^{\frac{1}{2}}  \mean{  \partial _{n} v}      \|_{ \mathcal{F} _{h} \cap \Omega  }^{2  } \ge   - \varepsilon C_{I }  \|\nabla v  \|_{ \mathcal{T}_{h}  }^{2  } \ge -\varepsilon \frac{C}{2} ( \|\nabla v  \|_{ \Omega}^{2} +  \abs{
         v}  _{g_{h}  }^{  2}  ),  \\
    - \varepsilon \| h^{\frac{1}{2}}    \partial _{n} v      \|_{ \Gamma   }^{2  } \ge   - \varepsilon C_{I }  \|\nabla v  \|_{ \mathcal{T}_{h}  }^{2  } \ge -\varepsilon \frac{C}{2} ( \|\nabla v  \|_{ \Omega}^{2} +  \abs{ v }  _{g_{h}  }^{ 2 }  ). \\
     \end{split}
 \]
 Remark that this holds $\| \nabla v \|_{ \mathcal{T} _{h}\cap \Omega  }^{  }  = \| \nabla v \|_{ \Omega  }^{  } $. Hence, the proof is complete
% \todo[inline]{ So why does the norm use $ \| \nabla v \|_{ \mathcal{T} _{h}\cap \Omega  }^{  }$ and not $\|  \nabla v \|_{ \Omega  }^{  } $? }



\end{proof}

Another important recipe before we can use Lax-Milgram is that the we need to show that $A_{h} $ is a bilinear bounded operator.

\begin{lemma}
    $A_{h}$ is bounded, i.e.,
    $$ \left\lvert A_{h}( v,u) \right\rvert \lesssim \| v \|_{A_{h}  }^{  }     \| u \|_{A_{h}  }^{  } \quad  \forall v,u \in V _{h}, $$
    whenever $\beta $ is large enough. Moreover, for $v \in  H^{2}( \mathcal{T}_{h} ) + V_{h} $
    and $w \in V_{h}$ satisfies \[
    \left\lvert a_{h}( v,w) \right\rvert  \lesssim \| v \|_{ a_{h},* }^{  }  \| w \|_{ a_{h},* }^{  }
    \]
\end{lemma}

Again, here  slightly more detailed proof based on \cite{gurkan2019stabilized}.
\begin{proof}
We will divide the proof in two steps.

    \begin{enumerate}[label=\arabic*)]
        \item
The goal is to show that $\left\lvert A_{h}( v,u) \right\rvert \lesssim \| v \|_{A_{h}  }^{  }     \| u \|_{A_{h}  }^{  }  \forall v,u \in V_{h} $.
            By definition we know that \[
    \left\lvert A_{h}( v,u) \right\rvert \le  \left\lvert a_{h}( v,u)  \right\rvert  + \left\lvert g_{h}( v,u)  \right\rvert,
    \]
    thus, it is only necessary to show that each of those terms are bounded.

         First of all, by assumption does the ghost penalty satisfy the Cauchy-Schwarz inequality, that is,  \[
\left\lvert g_{h}( v,u)  \right\rvert \le \left\lvert v \right\rvert _{g_{h}} \left\lvert u \right\rvert _{g_{h}} \le \| v \|_{ A_{h} }^{  } \| u \|_{ A_{h} }^{  }.
    \]

    Similarly, we can prove that,      \[
        \begin{split}
    \left\lvert a_{h}( v,u)  \right\rvert & \le  \left\lvert( \nabla v, \nabla u)_{\mathcal{T} _{h} \cap \Omega }  \right\rvert +  \left\lvert  ( \partial _{n} v,u)_{\Gamma } \right\rvert   + \left\lvert ( v, \partial _{n} u)_{\Gamma } \right\rvert  +
    \left\lvert\beta ( h^{-1} v,u)_{\Gamma }  \right\rvert  \\
    & \quad  + \left\lvert( \mean{ \partial _{n} v }  , \jump{ u }  ) _{\mathcal{F} _{h} \cap \Omega }  \right\rvert  + \left\lvert ( \jump{ v }  , \mean{ \partial _{n} u }  )_{\mathcal{F} _{h}\cap \Omega } \right\rvert  + \left\lvert \beta ( h^{-1} \jump{ v }  ,
    \jump{ u }  )_{\mathcal{F}_{h}\cap \Omega  } \right\rvert \\
     & \lesssim  \| u \|_{ A_{h} }^{  } \| v \|_{ A_{h} }^{  }
        \end{split}
    \]

This is proved by showing that each term is bounded by applying the Cauchy-Schwartz inequality, Assumption \ref{as:est_EP1} and Corollary \ref{cor:g_h_inverste_results} and \ref{cor:unfitted_inverse_estimates}. That is, here is some
    detailed calculations,
    \begin{enumerate}
        \item The first term, \[
\begin{split}
\left\lvert( \nabla v, \nabla u)_{\mathcal{T} _{h} \cap \Omega }  \right\rvert & \le \| \nabla v \|_{ \mathcal{T} _{h}\cap \Omega  }^{  }\| \nabla u \|_{ \mathcal{T} _{h} \cap \Omega  }^{  } \\
& \le \| \nabla v \|_{ \mathcal{T} _{h}  }^{  }\| \nabla u \|_{ \mathcal{T} _{h}   }^{  } \\
        & \lesssim   ( \| \nabla v \|_{ \Omega  }^{  } + \abs{ v } _{g_{h}} )( \| \nabla u \|_{ \Omega  }^{  } + \abs{ u } _{g_{h}} )\\
        &  \lesssim  \| v \|_{ A_{h} }^{  } \|u  \|_{ A_{h} }^{  }.
\end{split}
            \]

        \item For the second and third term,
            \[
                \begin{split}
                 \left\lvert ( \partial _{n} v,  u)_{\Gamma } \right\rvert & \le \|h^{\frac{1}{2}} \partial _{n}v  \|_{ \Gamma   }^{  } \| h^{-\frac{1}{2}} u  \|_{ \Gamma  }^{  }  \\
                 & \le \| \nabla v \|_{\mathcal{T}_{h}   }^{  } \|h^{-\frac{1}{2}} u \|_{ \Gamma    }^{  } \\
 % &  \le ( \| \nabla v \|_{ \Omega  }^{  } + \abs{ v } _{g_{h}} ) \|h^{-\frac{1}{2}} u \|_{ \Gamma    }^{  } \\
                 & \lesssim   \| v \|_{A_{h}   }^{  } \| u \|_{ A_{h}    }^{  }.
                \end{split}
            \]

        \item The boundary penalty term,
            \[
                \begin{split}
                    \left\lvert\beta ( h^{-1} v,u)_{\Gamma }  \right\rvert  & \le \beta \| h^{-\frac{1}{2}} v \|_{\Gamma   }^{  } \| h^{-\frac{1}{2}} u \|_{\Gamma   }^{  } \\
                                                                            &\lesssim \| u \|_{\mathcal{A}_{h}   }^{  }\| v \|_{\mathcal{A}_{h}   }^{  }.
                \end{split}
            \]

        \item For the jump terms, \[
                \begin{split}
                    \left\lvert( \mean{ \partial _{n} v }  , \jump{ u }  ) _{\mathcal{F} _{h} \cap \Omega }  \right\rvert &\lesssim  \| h^{\frac{1}{2}} \mean{ \partial _{n} v }   \|_{ \mathcal{F}_{h}\cap \Omega   }^{  } \|h^{-\frac{1}{2}} \jump{ u }   \|_{ \mathcal{F}_{h}\cap
                    \Omega   }^{  } \\
&\lesssim  \| u   \|_{ A_{h}    }^{  } \|  u    \|_{ A_{h}   }^{  }.
                \end{split}
        \]

    \item And for the jump penalty term,
        \[
            \begin{split}
         \left\lvert \beta ( h^{-1} \jump{ v }  ,
         \jump{ u }  )_{\mathcal{F}_{h}\cap \Omega  } \right\rvert &\le  \beta \| h^{-\frac{1}{2}    }\jump{ v } \|_{ \mathcal{F}_{h} \cap \Omega  }^{  }\| h^{-\frac{1}{2}    }\jump{ u } \|_{\mathcal{F}_{h} \cap \Omega   }^{  } \\
    &\lesssim \| v \|_{A_{h}  }^{  } \| u \|_{A_{h}  }^{  }.
            \end{split}
        \]
        Hence, the statement $\left\lvert A_{h}( v,u) \right\rvert \lesssim \| v \|_{A_{h}  }^{  }     \| u \|_{A_{h}  }^{  }  $ holds $ \forall v,u \in V_{h} $.
    \end{enumerate}

\item We want to show $\abs{ a_{h}( v,u) }  \lesssim \| v \|_{ a_{h},* }^{  }  \| u \|_{ a_{h},* }^{  }$
for $ \forall v \in  H^{2}( \mathcal{T}_{h} ) + V_{h} $ and $ \forall u \in V_{h}$,
    that is,      \[
        \begin{split}
    \left\lvert a_{h}( v,u)  \right\rvert & \le  \left\lvert( \nabla v, \nabla u)_{\mathcal{T} _{h} \cap \Omega }  \right\rvert +  \left\lvert  ( \partial _{n} v,u)_{\Gamma } \right\rvert   + \left\lvert ( v, \partial _{n} u)_{\Gamma } \right\rvert  +
    \left\lvert\beta ( h^{-1} v,u)_{\Gamma }  \right\rvert  \\
    & \quad  + \left\lvert( \mean{ \partial _{n} v }  , \jump{ u }  ) _{\mathcal{F} _{h} \cap \Omega }  \right\rvert  + \left\lvert ( \jump{ v }  , \mean{ \partial _{n} u }  )_{\mathcal{F} _{h}\cap \Omega } \right\rvert  + \left\lvert \beta ( h^{-1} \jump{ v }  ,
    \jump{ u }  )_{\mathcal{F}_{h}\cap \Omega  } \right\rvert \\
     & \lesssim  \| v \|_{ a_{h},* }^{  }  \| u \|_{ a_{h},* }^{  }
        \end{split}
    \]
    The proof follows exactly the same steps as the first part. However, the only difference is that the inverse estimates in the Corollary \ref{cor:unfitted_inverse_estimates} is not needed since the terms is included in the norm, thus, \[
        \begin{split}
    \abs{ ( \partial _{n} v, u) _{\Gamma } } & \lesssim \| v \|_{ a_{h},* }^{  } \| u \|_{a_{h},*  }^{  } \\
    \abs{ ( \mean{ \partial _{n} v  }  , \jump{ u }  ) _{\mathcal{F} _{h} \cap \Omega  } } & \lesssim \| v \|_{ a_{h},* }^{  } \| u \|_{a_{h},*  }^{  }
        \end{split}
    \]

    \end{enumerate}

    Hence, the proof is complete.



\end{proof}

\subsection{A priori estimate}%
\label{sub:a_priori_estimate}

In this section will we do only a brief overview of the important assumptions and definitions required to prove optimal convergence.

Let us denote the $L^2$-orthogonal projection $\pi _{h}: L^2( \mathcal{T} _{h}) \to V_{h}$. A key idea is to distinguish between the physical space $\Omega $ and the polyhedra $\Omega ^{e}_{h} = \cup _{T\in \mathcal{T} _{h} } T$. Thus, to do an
a priori estimate we find it necessary to define an bounded extension operator satisfying, \[
( \cdot ) ^{e}: W^{m,q}( \Omega )  \to W^{m,q} ( \Omega ^{e}), \quad \| v^{e} \|_{ m,q,\Omega ^{e}  }^{  } \lesssim \| v \|_{ m,q, \Omega  }^{  }.
\]
where $0< m \le \infty$ and $1 \le q \le \infty$. For more information, see \cite{gurkan2019stabilized}.

Now assume that $\Omega _{h,e} \subset  \Omega_{e} $ for some $\Omega^{e}  $. We define an unfitted $L^2$-projection $\pi _{h}^{e}: H^{r}( \Omega ^{e}_{h}) \to V_{h}$ s.t.  $\pi ^{e} _{h} v := \pi _{h} v^{e}$.
\todo[inline]{ As Andre about the difference between $\Omega _{e}$, $\Omega ^{e}$  and $\Omega _{e,h}$. }
That is, we will do the following assumption before presenting the a priori estimate,
\begin{assumption}[Weak consistency estiamte]
    \label{as:weak_consistency_estimate}
    Let $v \in H^{s}( \Omega )  $  and $r = \min_{} \{ s, k+1 \}$.  Then we assume that \[
    \abs{ \pi ^{e}_{h} v } _{g_{h}} \lesssim  h^{r-1}\| v \|_{ r,\Omega  }^{  }
    \]

\end{assumption}

Finally, we have the notation we need to present the following estimate.

\begin{theorem}[A priori estimate]
    Let the Assumption \ref{as:weak_consistency_estimate} hold. If we let $u \in H^{s}( \Omega ) $, $s\ge 2$ be the exact solution to the weak formulation \eqref{eq:possion_weak} and let $u_{h} \in \mathcal{P}^{k}( \mathcal{T} _{h})  $ be the solution the problem \ref{def:cutdg_poisson_problem}. Then for $r =
    \min_{}\{s, k+1\} $ the error $e = u - u_{h}$ satisfies \[
        \begin{split}
    \| e \|_{ a_{h},* }^{  } \lesssim   h^{r-1} \| u \|_{ r,\Omega  }^{  }\\
    \| e \|_{ \Omega  }^{  } \lesssim   h^{r} \| u \|_{ r,\Omega  }^{  }\\
        \end{split}
    \]
\end{theorem}
\begin{proof}
    For a complete proof, see \cite{gurkan2019stabilized}.
\end{proof}


\subsection{Condition number estimates}%
\label{sub:condition_number_estimates}


An important consideration is to see what happens with the condition number when the mesh has barely intersected cut mesh element. In this section will we do some assumptions on the ghost penalty and show that the condition number is bounded by $O( h^{-2}) $ irrespectively on how $\Gamma $ intersects the active mesh $\mathcal{T}_{h} $.

Let $V_{h}$ be spanned by a polynomial basis  $\left\{ \phi _{i}  \right\}_{i=1}^{N} $ so that $v = \sum_{i=1}^{N} V_{i}\phi _{i}$ with coefficients $V= \left\{ V_{i} \right\} _{i=1}^{N}$. From basic finite element theory is the stiffness matrix
$\mathcal{A} $ defined s.t.,
\[
    ( \mathcal{A} V, W)  = A_{h}( v,w), \quad \forall v,w \in V_{h}
\]
where $ \left[ \mathcal{A} \right] _{ji} := A_{h}( \phi _{i}, \phi_{j} )$.
Since $A_{h}$ is well-posed is it well known that $\mathcal{A}: \mathbb{R} ^{N} \to \mathbb{R} ^{N} $ must be isomorphic \cite{ern2006evaluation}.  Thus, this linear system must holds $ \mathcal{A} V = f$, where$\left[ f \right] _{j} := l_{h}(\phi_{j} )$. That is, we define the operator norm as


\begin{equation}
\label{eq:operator_norm}
 \| \mathcal{A}  \|_{ \mathbb{R} ^{N}  }^{  } = \sup_{V \in \mathbb{R} ^{n} \setminus 0 } \frac{\| \mathcal{A} V \|_{ \mathbb{R} ^{N} }^{  } }{\| V \|_{ \mathbb{R} ^{N} }^{  } }
\end{equation}
\todo[inline]{ What is the definition of an $\| \cdot  \|_{ \mathbb{R} ^{N} }^{  } $  norm?  Is it what we know as $\| \cdot  \|_{ 2 }^{  } $?    }

 With a corresponding definition of condition number, \( \kappa ( \mathcal{A} )  = \| \mathcal{A}  \|_{ \mathbb{R} ^{N} }^{  } \| \mathcal{A}^{-1}  \|_{ \mathbb{R} ^{N} }^{  }    \).
 Here we denote $\| \cdot  \|_{ \mathbb{R} ^{N} }^{  } $  as the discrete $l^{2}$ norm.

 Following \cite{gurkan2019stabilized}, we need the following estimates to show that the condition number is conserved, that is, bounded by $O( h^{-2}) $. First of all, we need to find estimates which can bound the discrete $l^{2}$ norm to the active
 mesh norm  $\mathcal{T} _{h}$-norm \footnote{Here we denote the $\mathcal{T} _{h}$-norm as $\| \cdot  \|_{\mathcal{T} _{h}  }^{  } $, where $\mathcal{T} _{h}$ is the active mesh.}. This is not straight forward does, in fact, introduce some new
 assumptions on our ghost penalty. The idea is to first bound $l_{2}$-norm with the continuous $L^{2}$-norm, and similarly bound the $L^{2}$ -norm to the energy $A_{h}$-norm. Finally, since $A_{h}$ is consisting of the ghost penalty $g_{h}$ we add
 some assumptions to require that it can be inversely bounded to the $\mathcal{T} _{h}$-norm and, thus, makes to possible to close the gap between the $l^{2}$-norm and $\mathcal{T} _{h}$-norm.

 To be able to transform from the discrete $l^{2}$-norm to the continuous $L^{2} $-norm the following equivalence estimate is useful,
 \begin{proposition}
 \label{prop:inverse_relation}
     Let $\mathcal{T}_{h} $ be a quasi-uniform mesh and $v \in V^{h}$. Then the following relation holds,
 \begin{equation}
 h^{\frac{d}{2}} \| V \|_{ \mathbb{R} ^{N} }^{  }  \lesssim \| v \|_{ L^2( \mathcal{T} _{h})  }^{  } \lesssim h^{\frac{d}{2}} \| V \|_{\mathbb{R} ^{N}  }^{  }.
 \end{equation}
 \end{proposition}
 \begin{proof}
    \todo[inline]{ TODO: Need to find source. Maybe in \cite{ern2006evaluation}. }
 \end{proof}

 Since we can now transfer information from the discrete $l^{2} $-norm to $L^{2}$-norm. We now want to connect the discrete norm to the energy norm, $A_{h}$. This can be fulfilled using an extension on the ghost penalty, that is,

 \begin{assumption}[EP3]
     \label{as:est_EP3}
 \[
 \| v \|_{ \mathcal{T}_{h}  }^{  } \le  C \| v \|_{ \Omega  }^{  }  + \abs{ v } _{g_{h}}
 \]
 Where the constant only depends on the shape-regularity, the dimension $d$ and the order $k$.
 \end{assumption}


 \begin{proposition}[Discrete Poincare inequality]
     Let $\mathcal{T}_{h} $ be shape-regular and quasi uniform and assume \ref{as:est_EP3}. Then for $v \in V_{h}$, it holds that \[
     \| v \|_{ \mathcal{T} _{h} }^{  } \lesssim \| v \|_{ A_{h} }^{  }
     \]

 \end{proposition}
\begin{proof}
    See \cite{gurkan2019stabilized}.
\end{proof}



We have now showed that bounded the discrete $l^{2}$- norm with the energy $A_{h}$ norm. The final recipe is to find the relation between the $l^{2}$-norm and the $L^{2}$-norm over the active mesh, which can be obtained by accepting these
inequalities;
\begin{equation}
\label{eq:zero_order_inverse_est}
    \| \nabla v \|_{ T\cap \Omega  }^{  }  \lesssim \| h^{-1} v \|_{  }^{  },\quad
    \|  v \|_{ \Gamma \cap \Omega  }^{  }  \lesssim \| h^{-\frac{1}{2}} v \|_{  }^{  } \text{  and  }
    \|  v \|_{ F \cap \Omega  }^{  }  \lesssim \| h^{-\frac{1}{2}} v \|_{  }^{  }.
\end{equation}
\todo[inline]{ TODO: proof }
Remark that these inverse estimates are closely related to the estimates discussed in \eqref{eq:unfitted_localinv}. One final assumption is to make sure the ghost penalty has a similar inverse estimate to the active mesh.
\begin{assumption}[EP4]
    \label{as:est_EP4}
    For $v \in V_{h}$, then \[
    \abs{ v } _{g_{h}} \lesssim h^{-1} \| v \|_{ \mathcal{T} _{h} }^{  }
    \]
\end{assumption}

In fact, this brings us to the following corollary.
\begin{corollary}[Inverse estimate for the energy norm ]
    \label{cor:inverse_energy_norm}
    For $v \in V_{h}$, \[
    \| v \|_{ A_{h} }^{  } \lesssim  h^{-1}\| v \|_{ \mathcal{T} _{h} }^{  }
    \]
\end{corollary}

Hence, we have finally arrived to close the gap between the $l^{2}$ norm and the $\mathcal{T} _{h}$ norm. Using this result can we finally show that the condition number is conserved.

\begin{theorem}[Condition number estimates]
    Let $\mathcal{A} $ be the corresponding stiffness matrix to $A_{h}$, then the condition number satisfies \[
    \kappa ( \mathcal{A} ) \lesssim O(h^{-2}).
    \]

\end{theorem}
\begin{proof}
    By definition is $\kappa ( \mathcal{A} ) = \| \mathcal{A}  \|_{ \mathbb{R} ^{N} }^{  } \| \mathcal{A}^{-1}  \|_{ \mathbb{R} ^{N} }^{  }$. The goal is to show that each term $\| A \|_{ \mathbb{R} ^{N} }^{  } $   and $\| A^{-1} \|_{\mathbb{R} ^{N}
    }^{  } $ is bounded and, thus, dividing the proof into two parts.

    First of all, we want to establish a interesting result. Applying the Corollary \ref{cor:inverse_energy_norm} and Proposition \ref{prop:inverse_relation} can we immediately observe that
    \begin{equation}
    \label{eq:inverse_equivalence_relation}
    \|w \|_{ A_{h} }^{  } \lesssim h^{-1} \| w \|_{ \mathcal{T} _{h} }^{  } \lesssim h^{\frac{( d-2) }{2} }\| W \|_{\mathbb{R} ^{N}  }^{  }.
    \end{equation}
    \todo[inline]{ Maybe discuss a line before the theorem that $ h^{-1} \| w \|_{ \mathcal{T} _{h} }^{  } \lesssim h^{\frac{( d-2) }{2} }\| W \|_{\mathbb{R} ^{N}  }^{  }$ should hold. Is the quivalence "big" enough to also bound $h^{-1}\| w \|_{
    \mathcal{T} _{h} }^{  } $? }


    \begin{enumerate}[label=\arabic*)]
        \item  The goal is to show that $\| \mathcal{A}  \|_{ \mathbb{R} ^{N} }^{  } $ is bounded. This can be done is the following manner.
    \[
        \begin{split}
            \| \mathcal{A} V \|_{ \mathbb{R} ^{N} }^{  } &= \sup_{W \in \mathbb{R} ^{N} \setminus 0} \frac{( \mathcal{A} V, W) _{\mathbb{R} ^{N}}}{\| W \|_{ \mathbb{R} ^{N} }^{  } } = \sup_{w \in  V_{h} \setminus 0} \frac{A_{h} ( v,w) \| w \|_{ A_{h} }^{  } }{\| w \|_{ A_{h}  }^{  } \| W \|_{\mathbb{R} ^{N}  }^{  }  } \\
                                                         &\lesssim \sup_{w \in  V_{h} \setminus 0} \frac{A_{h} ( v,w) }{\| w \|_{ A_{h}  }^{  }} h^{\frac{d-2}{2} }   \\
    &\lesssim h^{\frac{( d-2) }{2}  } \| v \|_{ A_{h}  }^{  } \lesssim h^{d-2} \| V \|_{ \mathbb{R} ^{N} }^{  }
        \end{split}
    \]
Here the first equality is served by the definition \eqref{eq:operator_norm}.
\todo[inline]{ Which is disagree on. }
The second equality is a result of the definition $( \mathcal{A} V,W) = A_{h}(w,v)$.
\todo[inline]{ Which I also disagree on. Why are you allowed interchange the sup from $\mathbb{R} ^{N}$ to $V_{h}$ and still keep the equal sign.   }
The first inequality is basically an application of the equivalence estimate \eqref{eq:inverse_equivalence_relation}.
Using this result can we argue that \[
    \| \mathcal{A} V \|_{ \mathbb{R} ^{N}} = \sup_{V \in \mathbb{R} ^{N} \setminus 0} \frac{\| \mathcal{A} V \|_{ \mathbb{R} ^{N} }^{  } }{\| V \|_{ \mathbb{R} ^{N} }^{  } } \lesssim h^{d-2}
\]
And completing the first part of the proof.
\item The goal is to show that $\| \mathcal{A} ^{-1} \|_{ \mathbb{R} ^{N} }^{  } $  is bounded.
    \[
    \begin{split}
          \| V \|_{ \mathbb{R} ^{N} }^{ 2 } & \lesssim h^{-d} \| v \|_{ \mathcal{T}_{h}  }^{  }  \\
          & \lesssim  h^{-d} A_{h}( v,v) =   h^{-d} ( V, \mathcal{A} V)_{\mathbb{R} ^{N}} \\
          &  \lesssim  h^{-d}  \| V \|_{ \mathbb{R} ^{N} }^{  } \| \mathcal{A} V \|_{ \mathbb{R} ^{N} }^{  }
    \end{split}
    \]
    \todo[inline]{ Where does $h^{-d}$ come from??  }
    \todo[inline]{ TODO: explain all steps.  }
and hence $\| V \|_{\mathbb{R} ^{N}   }^{  } \lesssim h^{-d} \| AV \|_{\mathbb{R} ^{N}  }^{  }  $. And now setting $V= \mathcal{A} ^{-1} W$ we conclude that $\| \mathcal{A} ^{-1} \|_{ \mathbb{R} ^{N} }^{  } \lesssim h^{-d} $
    \end{enumerate}

    Thus, the original statement $\kappa ( \mathcal{A} )  = \| \mathcal{A}  \|_{ \mathbb{R} ^{N} }^{  } \| \mathcal{A}^{-1}  \|_{ \mathbb{R} ^{N} }^{  } \lesssim h^{d-2} h^{-d} = h^{-2}$ holds and the proof is complete.


\end{proof}



\subsection{Constructing ghost penalties}%
\label{sub:constructing_ghost_penalties}

Here is a complete list of the assumptions we have done on $g_{h}$.

\begin{enumerate}[label=(\roman*)]
\item \textbf{EP1}.
    $H^{1}$ semi-norm extension property for $v \in  V_{h}$,
    \begin{equation}
        \label{eq:EP1}
    \| \nabla v \|_{ \mathcal{T} _{h} }^{  }  \lesssim \| \nabla v \|_{ \Omega  }^{  }  + \abs{ v } _{g_{h}}
    \end{equation}
\item \textbf{EP2}.
    Weak consistency for $v \in H^{s}( \Omega ) $ and $r = \min\{s, k+1\} $,
    \begin{equation}
        \label{eq:EP2}
    \abs{ \pi ^{e}_{h} v }_{g_{h}} \lesssim  h^{r-1} \| v \|_{ r,\Omega  }^{  }
    \end{equation}

\item \textbf{EP3}.
    $L^{2}$ norm extension property for $v \in V_{h}$,
    \begin{equation}
    \label{eq:EP3}
    \| v \|_{ \mathcal{T} _{h} }^{  } \lesssim \| v  \|_{\Omega   }^{  }   + \abs{ v } _{g_{h}}
    \end{equation}
    \item\textbf{EP4}.
    Inverse inequality for $v \in  V_{h}$,
    \begin{equation}
    \label{eq:EP4}
    \abs{ v } _{g_{h}} \lesssim h^{-1} \| v \|_{ \mathcal{T} _{h} }^{  } .
    \end{equation}

\end{enumerate}

\todo[inline]{TODO: Define norm $\| \cdot  \|_{ r,\Omega  }^{  } $ in mathematical background.  }

The goal in this chapter is to engineer an ghost penalty which fulfills these assumptions.
Let us introduce the notation \[
\partial _{n}^{j} v = \sum_{\abs{ \alpha  } =1 }^{} \frac{D^{\alpha }v( x) n^{\alpha }}{\alpha !}, \quad \abs{ \alpha  } = \sum_{j}^{d} \alpha_{i}
\]
where we denote the multi-index $\alpha  = ( \alpha _{1}, \ldots, \alpha _{d})  $  and $n^{\alpha } = n_{1}^{\alpha _{1}} \ldots n_{d}^{\alpha _{d}}$. An useful result that may help us design ghost penalty is the following estimate.
\begin{lemma}
    \label{lemma:local_facet_estimate}
    Let $T_{1}, T_{2} \in  \mathcal{T} _{h} $ share a common facet $F \in \mathcal{F}_{h} $. Then for $v_{h} \in  V_{h}$  does this hold \[
    \| v \|_{ T_{1} }^{  }  \lesssim  \| v \|_{ T_{2} }^{  }  + \sum_{j=0}^{k}  h^{2j +1} ( \jump{ \partial _{n}^{j} v}, \jump{ \partial _{n}^{j} v}    )_{F}
    \]
\end{lemma}

\begin{proof}
    For a detailed proof, see \cite{gurkan2019stabilized}.
\end{proof}

We will now introduce the so-called ghost penalty faces, that is, \[
\mathcal{F} ^{g}_{h} = \left\{ F\in \mathcal{F} _{h} : T^{+}\cap \Gamma \neq \emptyset  \vee T^{-}\cap \Gamma \neq \emptyset  \right\}.
\]
\todo[inline]{
    This definition seems not precise. Do do you mean this?

    \[
\mathcal{F} ^{g}_{h} = \left\{ F\in \mathcal{F} _{h} : T^{+}\cap \Gamma \neq \emptyset  \text{ and } T^{-}\cap \Gamma \neq \emptyset    \text{ for } T^{+}, T^{-} \in \mathcal{T} _{h} \right\}.
\] and does also not consist with the drawing \cite[Fig 2.1]{gurkan2019stabilized}.
  }
\todo[inline]{ Is $\mathcal{F} _{h}^{g}$  equivalent with all facets inside $\mathcal{T}_{\Gamma }? $ }
This set is simply all facets that belong to all elements of the active mesh $\mathcal{T} _{h}$  intersected with $\Gamma $.

\begin{lemma}
    \label{lemma:inv_gh_lemma}
    Assume \ref{as:G2} and \ref{as:G3}.
    For $v \in  V_{h}$ it holds that
        \begin{align}
            \label{eq:inv_gh_1}
        \| v \|_{ \mathcal{T} _{h} }^{ 2 }  & \lesssim  \| v \|_{ \Omega  }^{ 2 }  + \sum_{j=0}^{k} h^{2j+1} ( \jump{ \partial ^{j}_{n} v }, \jump{ \partial ^{j}_{n} }  v  )_{\mathcal{F}_{h}^{g}}\\
            \label{eq:inv_gh_2}
        \| \nabla v \|_{ \mathcal{T} _{h} }^{ 2 }  & \lesssim  \| \nabla v \|_{ \Omega  }^{ 2 }  + \sum_{j=0}^{k} h^{2j-1} ( \jump{ \partial ^{j}_{n} v }, \jump{ \partial ^{j}_{n} }  v  )_{\mathcal{F}_{h}^{g}}
        \end{align}

\end{lemma}

\begin{proof}
    We will dive the proof in two parts where we first prove \eqref{eq:inv_gh_1} and then in the second part prove \eqref{eq:inv_gh_2}.
    \begin{enumerate}[label=\arabic*)]
        \item We want to show that
            \[
        \| v \|_{ \mathcal{T} _{h} }^{ 2 }   \lesssim  \| v \|_{ \Omega  }^{ 2 }  + \sum_{j=0}^{k} h^{2j+1} ( \jump{ \partial ^{j}_{n} v }, \jump{ \partial ^{j}_{n} }  v  )_{\mathcal{F}_{h}^{g}}.
            \]


        \item We want to show that
            \[
        \| \nabla v \|_{ \mathcal{T} _{h} }^{ 2 }   \lesssim  \| \nabla v \|_{ \Omega  }^{ 2 }  + \sum_{j=0}^{k} h^{2j-1} ( \jump{ \partial ^{j}_{n} v }, \jump{ \partial ^{j}_{n} }  v  )_{\mathcal{F}_{h}^{g}}
            \],
            Let us define the following norm \[
            g_{F_{i}}^{L^{2}}( v,v)  = \sum_{j=1}^{k} h^{2j+1}( \jump{ \partial ^{j}_{n}v }, \jump{ \partial ^{j}_{n}v }    )_{F_{i} }
            \]
            where $F_{i} \in  \mathcal{F} ^{g}_{h}$. Using Lemma \ref{lemma:local_facet_estimate} can we see that \[
            \| v \|_{ T_{i} }^{  } \lesssim \| v \|_{ T_{i+1} }^{ 2 } + g_{F_{i}}^{L^{2}}( v,v).
            \]
    Consequently, using induction over each pair $\left\{ T_{i}, T_{i+1} \right\} $ with a corresponding $F_{i}$, we obtain
            \[
                \begin{split}
            \| v \|_{ T_{1} }^{  }  & \le  C( \| v \|_{ T_{2} }^{ 2 } + g_{F_{1}}^{L^{2}}( v,v)\\
              & \le  C( C( \| v \|_{ T_{3} }^{ 2 } + g_{F_{2}}^{L^{2}}( v,v) ) + g_{F_{1}}^{L^{2}}( v,v) )\\
              & \lesssim    \| v \|_{ T_{k} }^{ 2 }  + \sum_{l=1}^{k-1} g_{F_{l}}^{L^{2}}( v,v)  \\
              & \lesssim    \| v \|_{ T_{k} \cap \Omega  }^{ 2 }  + \sum_{l=1}^{k-1} g_{F_{l}}^{L^{2}}( v,v)
                \end{split}
            \]
            Where the last steps arise from the fact that $\| \nabla v \|_{ T_{k} }^{ 2 } \le \| \nabla v \|_{ T_{k} \cap \Omega  }^{  }  $ which is a consequence of the Assumption \ref{as:G3}.
            \todo[inline]{ Okay, this inequality does not make sense. } Now doing a summation over the active mesh $\mathcal{T} _{h}$ we see that \[
                \begin{split}
                    \| v \|_{ \mathcal{T} _{h} }^{2  } &\lesssim \| v \|_{ \mathcal{T} _{h}\cap \Omega  }^{2  }+ \sum_{l=1}^{k-1} g_{F_{l}}^{L^{2}}( v,v) \\
                     &\lesssim \| v \|_{ \Omega  }^{ 2 }  + \sum_{j=0}^{k} h^{2j+1} ( \jump{ \partial ^{j}_{n} v }, \jump{ \partial ^{j}_{n} }  v  )_{\mathcal{F}_{h}^{g}}
                \end{split}
        \]
        \todo[inline]{ The first inequality is wrong. Fill in proof. }
        Hence, the first part of the proof is complete.

    \item We will simply start by replacing $v$  by $\nabla v$, that it,
        \[
                    \| v \|_{ \mathcal{T} _{h} }^{2  } \lesssim \| v \|_{ \Omega  }^{ 2 }  + \sum_{j=0}^{k} h^{2j+1} ( \jump{ \nabla  \partial ^{j}_{n} v }, \jump{ \nabla  \partial ^{j}_{n} }  v  )_{\mathcal{F}_{h}^{g}}
        \]
        Let us denote the tangential operator $P_{F} := I - n_{F} \oplus n_{F} $. This can be applied to decompe the gradient operator s.t. \[
        \nabla v = ( \partial _{n}v)n_{F} + P_{F} \nabla v.
        \]
        That is, applying the decomposition we get the following estimates \[
        \| \jump{ P_{F} }  \nabla \partial _{n}^{j} v \|_{ F }^{ 2 } = \| P_{F} \nabla \jump{ \partial _{n}^{j} v }   \|_{ F  }^{ 2} \lesssim h^{-2} \|  \jump{ \partial ^{j}_{n} v }   \|_{ F }^{  2}
        \]
        \todo[inline]{ Need to establish what inverse estimate that were applied. }
        and \[
        h^{2j +1} \| \partial ^{j}_{n} \nabla v  \|_{ F }^{  } \lesssim h^{2j +1 } \| \jump{ \partial ^{j+1}_{n} v }   \|_{F  }^{ 2 } + \| \jump{ \partial ^{j}_{n}v }   \|_{ F }^{ 2 }
        \]
        Thus, fulfilling \eqref{eq:inv_gh_2}.
        \todo[inline]{ Finish this proof. }

    \end{enumerate}


\end{proof}



Finally, we now have the tools we need to construct an candidate for the ghost penalty for which satisfies all assumptions.

\begin{proposition}[Face-based ghost penalty]
    For any set of positive parameters $\left\{ \gamma _{j} \right\} _{j=0}^{k}$, the ghost penalty defined as \[
    g^{1}_{h}( v,w)  := \sum_{j=0}^{k} \sum_{F \in \mathcal{F} _{h}^{g}}^{} \gamma _{j} h^{2j-1}_{F} ( \jump{ \partial ^{j}_{n} v }, \jump{ \partial ^{j}_{n} w }  ) _{F} \text{ for any } v,w \in V_{h},
    \]
    satisfies the assumptions described in the equations \eqref{eq:EP1}, \eqref{eq:EP2}, \eqref{eq:EP3} and \eqref{eq:EP4}.
\end{proposition}


\begin{proof}
    Using Lemma \ref{lemma:inv_gh_lemma} can we see that \eqref{eq:EP1} and \eqref{eq:EP3} is satisfied, that is \[
    \begin{split}
        \| \nabla v \|_{ \mathcal{T} _{h} }^{  } & \lesssim \| \nabla v \|_{ \Omega  }^{  }  + \abs{ v } _{g_{h}^{1}} \\
        \|  v \|_{ \mathcal{T} _{h} }^{  } & \lesssim \|  v \|_{ \Omega  }^{  }  + \abs{ v } _{g_{h}^{1}}
    \end{split}
    \]
    Thus, it now only reminds to show that $g_{h}^{1}$  satisfies \eqref{eq:EP2} and \eqref{eq:EP4} starting with the first on. As described in the assumption, let $v \in H^{s}( \Omega ) $ and $r = \min \{s, k+1\} $. Then it is easy to see that \[
        \begin{split}
        \abs{ \pi ^{e}_{h} v }_{g_{h}^{l}}^{2} & = \sum_{j=0}^{k} h^{2j-1} \| \jump{ \partial ^{j}_{n}\pi ^{e}_{h} v  }  \|_{\mathcal{F} _{h}^{g}  }^{2  } \\
        &= \sum_{j=0}^{r-1} h^{2j -1} \| \jump{ \partial ^{j}_{n} ( \pi _{h}^{e} v - v^{e}) }    \|_{\mathcal{F} _{h}^{g}  }^{  } +  \sum_{j=r}^{k} h^{2j-1} \| \jump{ \partial ^{j}_{n} \pi ^{e}_{h} v }   \|_{\mathcal{F} _{h}^{g}  }^{ 2 }    \\
        & \lesssim h^{2r- 2}\| v \|_{ r,\Omega  }^{ 2 } + h^{2j-2}\| D^{r} \pi ^{e}_{h} v \|_{ \mathcal{T} _{h} }^{ 2 }   \\
        & \lesssim h^{2r -2 } \| v \|_{ r,\Omega  }^{  }
        \end{split}
    \]
    Remark that we on the first sum in the second equality combined the fact that $\jump{ \partial _{n}^{j} v^{e} }| _{F} = 0 $ for $0\lesssim j \lesssim r-1$ and the approximation \[
    \| v-\pi _{h}^{e} v \|_{ \mathcal{F} _{h},r  }^{  } \lesssim h^{s-r -\frac{1}{2}} \| v \|_{ s,\Omega  }^{  }, \quad 0 \le r \le  s-\frac{1}{2}
\]. The proof of the approximation can be found in \cite[Chapter 2.41]{gurkan2019stabilized}.
\todo[inline]{ Show approximation in detail. Maybe do a longer recap of a priori results.}

The second sum was carried by the following inverse estimate \[
\| \partial ^{j}_{n}v \|_{ F }^{  }  \lesssim h^{r - j - \frac{1}{2}} \| D^{r} v \|_{ T }^{  }, \quad  \forall v \in V^{h}.
\]

and the stability of the projection operator $\pi _{h}$ and the Sobolev extension in the $H^{r}$ norm for $r=0$.
\end{proof}


